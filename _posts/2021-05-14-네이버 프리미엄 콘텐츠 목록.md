---
layout: post
title: "[파이썬] 네이버 프리미엄 콘텐츠 목록 긁어오기"
author: "asadal"
tags: ["python", "파이썬", "스크랩", "네이버", "프리미엄", "유료화", "크롤링"]
comments: true
use_math: true
---

네이버가 콘텐츠 유료 구독 모델 ['프리미엄 콘텐츠'](https://contents.premium.naver.com/) 서비스를 5월13일 공개했다. 지불할 가치가 있는 콘텐츠를 모아놓고 네이버 이용자에게 유료 구독을 유도하는 서비스다. 기존 언론사(와 자회사)를 포함해 전문 콘텐츠 제공사 25곳이 1차로 참여했다. 월 2900원부터 1만9900원까지 구독료도 다양하게 책정돼 있다. 오픈 한 달 동안은 무료 이벤트를 진행 중. 

주요 콘텐츠 제공업체(CP)와 브랜드, 구독료 등을 긁어와 한눈에 볼 수 있도록 CSV 파일로 저장해주는 파이썬 코드를 만들었다. 매체명, 매체 설명, 구독료, URL, 회사명을 뽑아와 CSV 파일로 저장해주는 코드다. 회사명과 구독료는 매체별 상세 페이지에 나와 있기에 이를 스크랩하는 함수를 따로 만들었다. 파이썬 초보라도 조금만 들여다보면 쉽게 짤 수 있는 수준이지만, 복습 차원에서 기록해 둔다.

```python
import requests
from bs4 import BeautifulSoup
from requests.api import get
import os
import csv

url = "https://contents.premium.naver.com/"
BASE_FOLDER = "/Users/asadal/Documents/Dev/"
filename = "arko.txt"

def get_company_name(url): 
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "lxml")
    company_name = soup.find("dd", attrs={"class": "business_item_info_text"}).text
    try:
        monthly_price =soup.find("em", attrs={"class": "subscription_item_sub_price"}).text
    except:
        monthly_price = "N/A"
    return [company_name, monthly_price]

def get_premium_info(html):
    brand = html.find("h2", attrs={"class": "pcc_name"}).text
    desc = html.find("p", attrs={"class": "pcc_desc"}).text
    link = url + html.find("a", attrs={"class": "pcc_link"})["href"]
    company = get_company_name(link)[0]
    price = get_company_name(link)[1]
    # print(brand, desc, price, link, company)
    return [brand, desc, price, link, company]

def get_premium_infos():
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "lxml")
    infos = soup.find_all("div", attrs={"class": "pcc_title_inner"})
    with open(BASE_FOLDER + "naver_premium.csv", mode="w+") as file:
        writer = csv.writer(file)
        writer.writerow(["매체명", "설명", "금액", "URL", "회사명"])
        for info in infos:
            brand_info = get_premium_info(info)
            writer.writerow(brand_info)

get_premium_infos()
```

[2021년 5월23일 업데이트] 기업 정보까지 긁어올 수 있도록 코드 수정.

```python
# import time
import requests
from bs4 import BeautifulSoup
from requests.api import get
# import telegram
import os
import csv

base_url = "https://contents.premium.naver.com/"
BASE_FOLDER = "/Users/asadal/Documents/Dev/"
filename = "arko.txt"

# 가격 정보 확인
def get_price(url):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "lxml")
    try:
        monthly_price =soup.find("em", attrs={"class": "subscription_item_sub_price"}).text
    except:
        monthly_price =soup.find("em", attrs={"class": "thumb_emphasis_promotion"}).text
    return monthly_price

# 개별 페이지에서 기업 정보 확인
def get_company_info(url):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "lxml")
    try:
        comp_info = soup.find("dl", attrs={"class": "product_info_list"}).text.strip()
    except:
        comp_info = soup.find("dl", attrs={"class": "business_info_list"}).text.strip()
    return comp_info

# 기업 정보 확인
def get_company_infos(url):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "lxml")
    try:
        comp_link = base_url + soup.find("div", attrs={"class": "subscription_item_inner"}).find("a")["href"]
    except:
        comp_link = base_url + soup.find("div", attrs={"class": "thumb_emphasis_inner"}).find("a")["href"]
    # print(comp_link)
    company = get_company_info(comp_link)
    return company

# 기업별 크롤링
def get_premium_info(html):
    brand = html.find("h2", attrs={"class": "pcc_name"}).text
    desc = html.find("p", attrs={"class": "pcc_desc"}).text
    link = base_url + html.find("a", attrs={"class": "pcc_link"})["href"]
    price = get_price(link)
    company = get_company_infos(link)
    # print(brand, desc, price, link, company)
    return [brand, desc, price, link, company]

# 최종 크롤링 함수
def get_premium_infos():
    res = requests.get(base_url)
    soup = BeautifulSoup(res.text, "lxml")
    infos = soup.find_all("div", attrs={"class": "pcc_title_inner"})
    with open(BASE_FOLDER + "naver_premium.csv", mode="w+") as file:
        writer = csv.writer(file)
        writer.writerow(["매체명", "설명", "금액", "URL", "회사 정보"])
        for info in infos:
            brand_info = get_premium_info(info)
            writer.writerow(brand_info)

get_premium_infos()
```

